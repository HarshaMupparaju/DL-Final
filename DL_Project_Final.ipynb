{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshaMupparaju/DL-Final/blob/main/DL_Project_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72G2ifRfp5E-"
      },
      "source": [
        "#Deep learning : Final Submission\n",
        "\n",
        "Sohan Reddy A : 2019A7PS0168G\n",
        "\n",
        "Sudeep Kumar Nemani : 2019A7PS0163G\n",
        "\n",
        "Sai Harsha Mupparaju : 2019A8PS0583G\n",
        "\n",
        "(Upload Kaggle.json before running)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bHmUZpbI5-c"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZKBzuwaHp8PW",
        "outputId": "1faa03c4-430a-4a87-f6d4-0f2030213243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: d2l in /usr/local/lib/python3.7/dist-packages (0.17.5)\n",
            "Requirement already satisfied: pandas==1.2.4 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.2.4)\n",
            "Requirement already satisfied: requests==2.25.1 in /usr/local/lib/python3.7/dist-packages (from d2l) (2.25.1)\n",
            "Collecting matplotlib==3.5.1\n",
            "  Using cached matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: numpy==1.21.5 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.21.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (4.10.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (7.7.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.3.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (21.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (4.33.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4->d2l) (2022.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.5.1->d2l) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->d2l) (1.15.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.2.1.post0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (5.4.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (1.1.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (2.15.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (21.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.11.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (3.8.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter==1.0.0->d2l) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter==1.0.0->d2l) (2.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.0.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.6.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.1.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.1.3\n",
            "    Uninstalling matplotlib-3.1.3:\n",
            "      Successfully uninstalled matplotlib-3.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 1.0.61 requires fastprogress>=0.2.1, but you have fastprogress 0.1.20 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-3.5.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib==3.1.3\n",
            "  Using cached matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.21.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.3) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.1.3) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.15.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.5.1\n",
            "    Uninstalling matplotlib-3.5.1:\n",
            "      Successfully uninstalled matplotlib-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 1.0.61 requires fastprogress>=0.2.1, but you have fastprogress 0.1.20 which is incompatible.\n",
            "d2l 0.17.5 requires matplotlib==3.5.1, but you have matplotlib 3.1.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-3.1.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastprogress==0.1.20 in /usr/local/lib/python3.7/dist-packages (0.1.20)\n",
            "Tesla K80\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l\n",
        "!pip install matplotlib==3.1.3\n",
        "!pip install fastprogress==0.1.20\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from d2l import torch as d2l\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader,ConcatDataset\n",
        "#from sympy.combinatorics.subsets import Subset\n",
        "import cv2\n",
        "\n",
        "\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "pd.set_option('display.max_columns', 20)\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2Kzr2QHqDoR",
        "outputId": "68a2c988-54ee-4cb2-84f6-7c3ab60872ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.25.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=d5b32537c0ef346f0ac60f0cb413c22fcd7b647df0754984ea53a0c24729a71f\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n",
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 99% 281M/285M [00:02<00:00, 116MB/s]\n",
            "100% 285M/285M [00:02<00:00, 124MB/s]\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLWt2HpWqKh-",
        "outputId": "3154d32a-ac1c-4f98-ce0f-115b550a1464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      "
          ]
        }
      ],
      "source": [
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge.zip   #Downloading FER2013 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m8HyZLn_qN93",
        "outputId": "68017b06-0a2f-4d98-81ba-62162f148e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fer2013/fer2013.csv\n",
            "fer2013/README\n",
            "fer2013/fer2013.bib\n",
            "fer2013/\n"
          ]
        }
      ],
      "source": [
        "! tar -xzvf \"/content/fer2013.tar.gz\"   #Extracting the FER2013 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAiQU6QYqS6m"
      },
      "source": [
        "##Preparing selected dataset\n",
        "\n",
        "Dataset : FER2013\n",
        "\n",
        "In the dataset, the following are given :\n",
        "\n",
        "- Training : for training the model\n",
        "\n",
        "- PublicTest : for validation\n",
        "\n",
        "It contains 3 columns, one for image, one for its corresponing emotion, and one for usage i.e, train or test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QbdEALc8qTvF",
        "outputId": "32ade6c7-e8e8-4dc7-a6c0-c62a328dff7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35887 entries, 0 to 35886\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   emotion  35887 non-null  int64 \n",
            " 1   pixels   35887 non-null  object\n",
            " 2   Usage    35887 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 841.2+ KB\n"
          ]
        }
      ],
      "source": [
        "dataset = pd.read_csv('./fer2013/fer2013.csv')\n",
        "dataset.info()                                        #Reading FER2013 Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ynGv9QE4qXYd",
        "outputId": "4ee3a745-04b3-485a-c4dc-0632ba811f2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Training       28709\n",
              "PublicTest      3589\n",
              "PrivateTest     3589\n",
              "Name: Usage, dtype: int64"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.Usage.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M0flBfcaqe5Q"
      },
      "outputs": [],
      "source": [
        "emotions = {\n",
        "    0: 'Angry', \n",
        "    1: 'Disgust', \n",
        "    2: 'Fear', \n",
        "    3: 'Happy', \n",
        "    4: 'Sad', \n",
        "    5: 'Surprise', \n",
        "    6: 'Neutral'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gvt-e4AIqhD0",
        "outputId": "6757f5bb-3412-4c26-81b0-4fffb287ec0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
            "       emotion                                             pixels       Usage\n",
            "28709        0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...  PublicTest\n",
            "28710        1  156 184 198 202 204 207 210 212 213 214 215 21...  PublicTest\n",
            "28711        4  69 118 61 60 96 121 103 87 103 88 70 90 115 12...  PublicTest\n",
            "28712        6  205 203 236 157 83 158 120 116 94 86 155 180 2...  PublicTest\n",
            "28713        3  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...  PublicTest\n",
            "...        ...                                                ...         ...\n",
            "32292        3  0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 3 4 21 40 53 65 ...  PublicTest\n",
            "32293        4  178 176 172 173 173 174 176 173 166 166 206 22...  PublicTest\n",
            "32294        3  25 34 42 44 42 47 57 59 59 58 54 51 50 56 63 6...  PublicTest\n",
            "32295        4  255 255 255 255 255 255 255 255 255 255 255 25...  PublicTest\n",
            "32296        4  33 25 31 36 36 42 69 103 132 163 175 183 187 1...  PublicTest\n",
            "\n",
            "[3588 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "train_df = dataset[dataset['Usage']=='Training']\n",
        "valid_df = dataset[dataset['Usage']=='PublicTest']\n",
        "test_df = dataset[dataset['Usage']=='PrivateTest']\n",
        "print(train_df.head())\n",
        "print(valid_df.head(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GIxyjZXgqjoX",
        "outputId": "3377fc56-92e0-48f3-97ce-d6d29bf0d485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   emotion                                             pixels        Usage\n",
            "0        0  170 118 101 88 88 75 78 82 66 74 68 59 63 64 6...  PrivateTest\n",
            "1        5  7 5 8 6 7 3 2 6 5 4 4 5 7 5 5 5 6 7 7 7 10 10 ...  PrivateTest\n",
            "2        6  232 240 241 239 237 235 246 117 24 24 22 13 12...  PrivateTest\n",
            "3        4  200 197 149 139 156 89 111 58 62 95 113 117 11...  PrivateTest\n",
            "4        2  40 28 33 56 45 33 31 78 152 194 200 186 196 20...  PrivateTest\n",
            "   -----   -------    -------    --------     -----    -------\n",
            "   emotion                                             pixels       Usage\n",
            "0        0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...  PublicTest\n",
            "1        1  156 184 198 202 204 207 210 212 213 214 215 21...  PublicTest\n",
            "2        4  69 118 61 60 96 121 103 87 103 88 70 90 115 12...  PublicTest\n",
            "3        6  205 203 236 157 83 158 120 116 94 86 155 180 2...  PublicTest\n",
            "4        3  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...  PublicTest\n"
          ]
        }
      ],
      "source": [
        "valid_df = valid_df.reset_index(drop=True) \n",
        "test_df = test_df.reset_index(drop = True)\n",
        "print(test_df.head())\n",
        "print('   -----   -------    -------    --------     -----    -------')\n",
        "print(valid_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl2ligQVqmCE"
      },
      "outputs": [],
      "source": [
        "pixels = []\n",
        "\n",
        "for pix in dataset.pixels:\n",
        "    values = [int(i) for i in pix.split()]\n",
        "    pixels.append(values)\n",
        "\n",
        "pixels = np.array(pixels)\n",
        "\n",
        "# rescaling pixel values\n",
        "pixels = pixels/255.0\n",
        "\n",
        "\n",
        "dataset.drop(columns=['pixels'], axis=1, inplace=True)\n",
        "\n",
        "pix_cols = [] # for keeping track of column names\n",
        "\n",
        "# add each pixel value as a column\n",
        "for i in range(pixels.shape[1]):\n",
        "    name = f'pixel_{i}'\n",
        "    pix_cols.append(name)\n",
        "    dataset[name] = pixels[:, i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YsC4ntCqqPA"
      },
      "source": [
        "###Checking how transformation works with our images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOORZxn7qvKs"
      },
      "outputs": [],
      "source": [
        "def show_example(df, num):\n",
        "    print('expression: ' ,df.iloc[num] )\n",
        "    image = np.array([[int(i) for i in x.split()] for x in df.loc[num, ['pixels']]])\n",
        "    print(image.shape)\n",
        "    image = image.reshape(48,48)\n",
        "    plt.imshow(image, interpolation='nearest', cmap='gray')\n",
        "    plt.show()\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zc0XOihqzdc"
      },
      "outputs": [],
      "source": [
        "i = show_example(train_df, 107)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zHTZmmcq0On"
      },
      "outputs": [],
      "source": [
        "i = i.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvfydtYtq3fE"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToPILImage(),transforms.RandomHorizontalFlip(p=0.9), transforms.RandomRotation([-30,-30])])\n",
        "tensor_img = transform(i)\n",
        "plt.imshow(tensor_img, interpolation='nearest', cmap='gray')\n",
        "plt.show()                  #Showing an image from FER2013"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4X_rHKNq-E9"
      },
      "source": [
        "###Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EK1EnDtq_Cd"
      },
      "outputs": [],
      "source": [
        "class FERDataset(Dataset):                          #Class to create FER2013 Dataset\n",
        "    '''\n",
        "        Parse raw data to form a Dataset of (X, y).\n",
        "    '''\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.tensor_transform = transforms.ToTensor()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_id = int(row['emotion'])\n",
        "        img = np.copy(row[pix_cols].values.reshape(48, 48))\n",
        "        img.setflags(write=True)\n",
        "\n",
        "        if self.transform:\n",
        "            img = Image.fromarray(img)\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            img = self.tensor_transform(img)\n",
        "\n",
        "        return img, img_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXJtWtQxrL_1"
      },
      "source": [
        "Function to transform the images according to the paper.\n",
        "\n",
        "Rotating the image at angles [-15, -10, -5, 0, 5, 10, 15]\n",
        "Horizontal Flip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh3y9CFNrQAi"
      },
      "outputs": [],
      "source": [
        "def image_transformations(angle) -> (object, object):                             #Class to transform images based on the paper\n",
        "    '''\n",
        "        Return transformations to be applied.\n",
        "        Input:\n",
        "            None\n",
        "        Output:\n",
        "            train_tfms: transformations to be applied on the training set\n",
        "            valid_tfms: transformations to be applied on the validation or test set\n",
        "    '''\n",
        "\n",
        "    train_trans = [      \n",
        "        transforms.RandomCrop(48, padding=4, padding_mode='reflect'),     \n",
        "        transforms.RandomRotation([angle,angle]),\n",
        "        transforms.RandomAffine(\n",
        "            degrees=0,\n",
        "            translate=(0.01, 0.12),\n",
        "            shear=(0.01, 0.03),\n",
        "        ),\n",
        "        transforms.RandomHorizontalFlip(p=1),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        "\n",
        "    val_trans = [\n",
        "        transforms.ToTensor(), \n",
        "    ]\n",
        "\n",
        "    train_transformations = transforms.Compose(train_trans)\n",
        "    valid_tfms = transforms.Compose(val_trans)\n",
        "\n",
        "    return train_transformations, valid_tfms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF7iaMFQrVc6"
      },
      "source": [
        "##Function to get the training dataset from the whole dataset (Training and Validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6fUeE3eOEsV"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle                                          #Downloading entire caleb-A dataset\n",
        "! pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle datasets download -d jessicali9530/celeba-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU2HeBbiOGSu"
      },
      "outputs": [],
      "source": [
        "! unzip 'celeba-dataset.zip'                                  #Extracting entire caleb-A dataset  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyYq-nWjcSfJ"
      },
      "source": [
        "##Eye faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEjXbo4rOO4U"
      },
      "outputs": [],
      "source": [
        "#Extracting eye attribute images\n",
        "\n",
        "\n",
        "class EyeFacesDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \n",
        "        self.landmarks_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "    \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        #print(\"GET!\")\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.landmarks_frame.iloc[idx, 0])\n",
        "\n",
        "        image = cv2.imread(img_name)\n",
        "\n",
        "        sample = image\n",
        "\n",
        "        sample = Image.fromarray(np.uint8(sample)).convert('RGB')\n",
        "        \n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)1\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XY8kf3QWIrK"
      },
      "outputs": [],
      "source": [
        "eye_dataset = EyeFacesDataset(csv_file = '/content/list_attr_celeba.csv', \n",
        "                                root_dir= '/content/img_align_celeba/img_align_celeba')\n",
        "\n",
        "landmarks_frame = pd.read_csv('/content/list_attr_celeba.csv')                 # read the csv file with attributes\n",
        "eye_images_list  = landmarks_frame.iloc[:,[2,13,16,24]]                    # extract the column with eye attributes-(2-Arched_Eyebrows,13-Bushy_Eyebrows,16-Eyeglasses,24-Narrow_Eyes)\n",
        "#print(eye_images_list)\n",
        "eye_images_list_1 =  eye_images_list[eye_images_list == 1]     # select only the indices for eye (any of the four attribute val = 1)\n",
        " \n",
        "eye_images_list_1 = eye_images_list_1.dropna( axis=0,\n",
        "                how='all')\n",
        "print(eye_images_list_1)\n",
        "#print(eye_images_list)\n",
        "eye_images_index_1 = eye_images_list_1.index      # corresponding image indices related to eyes\n",
        "#print(eye_images_index)\n",
        "\n",
        "eye_images_list_2 =  eye_images_list[eye_images_list != 1] \n",
        "\n",
        "eye_images_list_2 = eye_images_list_2.dropna( axis=0, how='any')   #Corresponding indices of images not related to eyes.\n",
        "eye_images_index_2 = eye_images_list_2.index \n",
        "print(eye_images_list_2)\n",
        "type(eye_images_index_2)\n",
        "eye_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHiXIAcBcWjC"
      },
      "source": [
        "## EyePN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgY0vPG8PNm-"
      },
      "outputs": [],
      "source": [
        "#Extracting eye attribute images\n",
        "\n",
        "\n",
        "class EyePNDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, df, root_dir, transform=None):\n",
        "        \n",
        "        self.landmarks_frame = df\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_labels = []\n",
        "        for i in range(len(self.landmarks_frame)):\n",
        "          self.image_labels.append(self.landmarks_frame['imtype'].values[i])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "    \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        #print(\"GET!\")\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        num = self.landmarks_frame.iloc[idx, 4]\n",
        "        \n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.landmarks_frame.iloc[idx, 5])\n",
        "\n",
        "        # image_data = Image.open(img_name).convert(\"RGB\") # Convert image to RGB channels\n",
        "        \n",
        "        # TODO: Image augmentation code would be placed here\n",
        "        \n",
        "        # Resize and convert image to torch tensor \n",
        "        # image_data = self.image_transformation(image_data)\n",
        "        image = cv2.imread(img_name)\n",
        "\n",
        "        sample = image\n",
        "\n",
        "        sample = Image.fromarray(np.uint8(sample)).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        labels = torch.Tensor(self.image_labels)#prodTensor = color_map_tensor *orb_image_tensor\n",
        "#prodasNp = (prodTensor.permute(2, 0, 1) * 255).to(torch.uint8).numpy()\n",
        "#prodasNp\n",
        "\n",
        "        return sample, labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFE14OXTGKb_"
      },
      "outputs": [],
      "source": [
        "df = eye_images_list\n",
        "# df.head()\n",
        "df['imtype'] = 0                                                                #Setting imtype to 0 for all images\n",
        "df.iloc[eye_images_index_1, df.columns.get_loc('imtype')] = 1                   #Setting imtype to 1 images which are related to eyes\n",
        "\n",
        "pdf = df[df['imtype'] == 1]\n",
        "pdf = pdf.sample(n=15000)\n",
        "\n",
        "ndf = df[df['imtype'] == 0]\n",
        "ndf = ndf.sample(n=15000)\n",
        "image_df = pdf.append(ndf)  \n",
        "image_df['image_id'] = [str(x+1).zfill(6) + '.jpg' for x in image_df.index]      #Adding new column for image location\n",
        "image_df = image_df.sample(frac=1)                                               #Appending dataframes of both negative and positive responses and shuffling\n",
        "val_df = df.sample(n=3000)                                                       #Randomly sampling data from entire dataset\n",
        "\n",
        "val_df['image_id'] = [str(x+1).zfill(6) + '.jpg' for x in val_df.index]          #Adding new column for image location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jQ_decTlr-I"
      },
      "outputs": [],
      "source": [
        "image_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-BtJ3fPTnkh"
      },
      "outputs": [],
      "source": [
        "eyepn_dataset= EyePNDataset(df = image_df,                                                  #Checking if datasets are proper\n",
        "                                root_dir= '/content/img_align_celeba/img_align_celeba')\n",
        "val_dataset = EyePNDataset(df = val_df, \n",
        "                                root_dir= '/content/img_align_celeba/img_align_celeba') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga4WcjldmgnZ"
      },
      "outputs": [],
      "source": [
        "len(eyepn_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jcis7jz6aijL"
      },
      "outputs": [],
      "source": [
        "eyepn_dataset[16000][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUFtL0RQD0qw"
      },
      "source": [
        "##Local Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzQNWuKL6iMj"
      },
      "outputs": [],
      "source": [
        "df = eye_dataset                                        #Extracting Local features using ORB function\n",
        "img = df[0]\n",
        "img = np.array(img)\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "orb = cv2.ORB_create()\n",
        "kp1, des1 = orb.detectAndCompute(gray,None)\n",
        "\n",
        "\n",
        "# draw the detected key points\n",
        "orb_image = cv2.drawKeypoints(gray, kp1, img,(255,0,0))\n",
        "\n",
        "lfeimg = cv2.cvtColor(orb_image, cv2.COLOR_BGR2RGB)\n",
        "# show the image\n",
        "orb_image = cv2.resize(orb_image,(48,48))\n",
        "orb_image.shape\n",
        "#plt.imshow(orb_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6R994ppD73r"
      },
      "source": [
        "## Global Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4ncyU_3A459"
      },
      "outputs": [],
      "source": [
        "img = df[0]                                        \t\t\t#Extracting Global features using hog function\n",
        "img = df[0]\n",
        "img = np.array(img)\n",
        "resized_img = resize(img, (128*4, 64*4))\n",
        "fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(4, 4),\n",
        "                \tcells_per_block=(4, 4), visualize=True,multichannel=True)\n",
        "#hog_image\n",
        "resized_img = cv2.resize(hog_image, (48, 48))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(resized_img)\n",
        "#type(resized_img)\n",
        "print(hog_image.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvSDmhdxTItk"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxllY6Exllsy"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# #random.seed(0)\n",
        "# list_of_random_positive_items = random.choices(eye_images_index_1,k=15000)\n",
        "# list_of_random_negative_items = random.choices(eye_images_index_2,k=15000)\n",
        "# validation_items = random.choices(e)\n",
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# path_to_your_files = '/content/img_align_celeba/img_align_celeba'\n",
        "# copy_to_path = '/content/img_align_celeba/eyes'\n",
        "# os.mkdir(copy_to_path, 0o777)\n",
        "# os.mkdir(copy_to_path+'/positive', 0o777)\n",
        "# files_list = sorted(os.listdir(path_to_your_files))\n",
        "# file_names= list_of_random_positive_items\n",
        "\n",
        "# for curr_file in file_names:\n",
        "#     #print(curr_file)\n",
        "#     shutil.copyfile(os.path.join(path_to_your_files, str(curr_file+1).zfill(6)+\".jpg\"),\n",
        "#                     os.path.join(copy_to_path+'/positive', str(curr_file+1).zfill(6)+\".jpg\")) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGxVcJjX-5K1"
      },
      "outputs": [],
      "source": [
        "# os.mkdir(copy_to_path+'/negative', 0o777)\n",
        "# files_list = sorted(os.listdir(path_to_your_files))\n",
        "# file_names= list_of_random_negative_items\n",
        "\n",
        "# for curr_file in file_names:\n",
        "#     #print(curr_file)\n",
        "#     shutil.copyfile(os.path.join(path_to_your_files, str(curr_file+1).zfill(6)+\".jpg\"),\n",
        "#                     os.path.join(copy_to_path+'/negative', str(curr_file+1).zfill(6)+\".jpg\")) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOXo7R-RNffU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RkDiSP49g63"
      },
      "source": [
        "##datagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkI7sERQh0Q_"
      },
      "outputs": [],
      "source": [
        "def image_transformations(angle) -> (object, object):                               #Transformation of image based on paper\n",
        "    '''\n",
        "        Return transformations to be applied.\n",
        "        Input:\n",
        "            None\n",
        "        Output:\n",
        "            train_tfms: transformations to be applied on the training set\n",
        "            valid_tfms: transformations to be applied on the validation or test set\n",
        "    '''\n",
        " \n",
        "    train_trans = [      \n",
        "        transforms.RandomCrop(48, padding=4, padding_mode='reflect'),     \n",
        "        transforms.RandomRotation([angle,angle]),\n",
        "        transforms.RandomAffine(\n",
        "            degrees=0,\n",
        "            translate=(0.01, 0.12),\n",
        "            shear=(0.01, 0.03),\n",
        "        ),\n",
        "        transforms.RandomHorizontalFlip(p=1),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        "\n",
        "    val_trans = [\n",
        "        transforms.ToTensor(), \n",
        "    ]\n",
        "\n",
        "    train_transformations = transforms.Compose(train_trans)\n",
        "    valid_tfms = transforms.Compose(val_trans)\n",
        "\n",
        "    return train_transformations, valid_tfms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YXCg4-Dh0jL"
      },
      "outputs": [],
      "source": [
        "def get_train_dataset(dataframe: object,ang , transformation: bool=True) -> (object, object):   #Getting dataset\n",
        "    '''\n",
        "        Returns an object on EyePNDataset class\n",
        "        Input:\n",
        "            dataframe: object -> DataFrame object containing the whole data\n",
        "            transformation: bool [optional] ->  Apply transformations\n",
        "    '''\n",
        "\n",
        "    # extracts rows specific to Training, PublicTest\n",
        "    #dataframe = dataframe.loc[dataframe.Usage.isin(['Training', 'PublicTest'])]\n",
        "    # drop Usage column as it's no longer needed    \n",
        "    #dataframe = dataframe.drop('Usage', axis=1)\n",
        "\n",
        "    # split dataset into training and validation set\n",
        "    np.random.seed(42)  \n",
        "    #msk = np.random.rand(len(dataframe)) < 0.8\n",
        "\n",
        "    #train_df = dataframe[msk].reset_index()\n",
        "    #val_df = dataframe[~msk].reset_index()\n",
        "\n",
        "    # get transformations\n",
        "    if transformation:\n",
        "        train_tfms, valid_tfms = image_transformations(ang)\n",
        "    else:\n",
        "        train_tfms, valid_tfms = None, None\n",
        "    \n",
        "    # fetch dataset\n",
        "    train_eye_ds = EyePNDataset(df = image_df, \n",
        "                                root_dir= '/content/img_align_celeba/img_align_celeba', transform=train_tfms)\n",
        "    val_eye_ds = EyePNDataset(df = val_df, \n",
        "                                root_dir= '/content/img_align_celeba/img_align_celeba', transform=valid_tfms)\n",
        "    print(train_eye_ds[0])\n",
        "    return train_eye_ds, val_eye_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiGpCRUBh0z2"
      },
      "outputs": [],
      "source": [
        "def get_train_dataloader(dataframe: object, transformation=True, batch_size: int=16) -> (object, object): #Getting DataLoader\n",
        "    '''\n",
        "        Returns train and test dataloaders.\n",
        "        Input:\n",
        "            dataframe: dataset DataFrame object\n",
        "            batch_size: [optional] int\n",
        "        Output:\n",
        "            train_dl: train dataloader object\n",
        "            valid_dl: validation dataloader object\n",
        "    '''\n",
        "    # fetech train and validation dataset\n",
        "    train_ds, valid_ds = get_train_dataset(dataframe, -15, transformation=transformation)\n",
        "    for i in range(-2, 4):\n",
        "      t, _ = get_train_dataset(dataframe, 5*i, transformation=transformation)\n",
        "      train_ds = ConcatDataset([train_ds,t])\n",
        "    print(len(train_ds))\n",
        "    train_dl = DataLoader(train_ds, batch_size, shuffle=True, \n",
        "                     num_workers=0, pin_memory=True)\n",
        "    valid_dl = DataLoader(valid_ds, batch_size, \n",
        "                    num_workers=0, pin_memory=True)\n",
        "    \n",
        "    return train_dl, valid_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4URYS06lh1E-"
      },
      "outputs": [],
      "source": [
        "# def get_test_dataloader(dataframe: object, batch_size: int=128) -> object:\n",
        "#     '''\n",
        "#         Returns test set dataloaders.\n",
        "#         Input:\n",
        "#             dataframe: dataset DataFrame object\n",
        "#             batch_size: [optional] int\n",
        "#         Output:\n",
        "#             test_dl: test dataloader object\n",
        "#     '''\n",
        "#     # extracts rows specific to PrivateTest\n",
        "#     test_df = dataframe.loc[dataset.Usage.isin(['PrivateTest'])]\n",
        "\n",
        "#     # drop Usage column as it's no longer needed\n",
        "#     test_df = test_df.drop('Usage', axis=1)\n",
        "\n",
        "#     # get transformations same as validation set\n",
        "#     _, valid_tfms = image_transformations()\n",
        "    \n",
        "#     test_dataset = FERDataset(test_df, transform=valid_tfms)\n",
        "#     test_dl = DataLoader(test_dataset, batch_size, num_workers=3 , pin_memory=True)\n",
        "\n",
        "#     # move loader to GPU (class defined ahead)\n",
        "#     test_dl = DeviceDataLoader(test_dl, device)\n",
        "#     return test_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5kEk980iGar"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dl, valid_dl = get_train_dataloader(eye_dataset, batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX0NPHcLWdrc"
      },
      "outputs": [],
      "source": [
        "for data, label in valid_dl:\n",
        "    print(data.size())\n",
        "    print(label.size())\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDneIQyaPQQk"
      },
      "outputs": [],
      "source": [
        "len(train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jkCb7faPiZj"
      },
      "outputs": [],
      "source": [
        "59*512"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Densenet Model"
      ],
      "metadata": {
        "id": "OmgFGrCtT8t_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYvOfn4c2t6r"
      },
      "outputs": [],
      "source": [
        "\n",
        "                          ##Densenet Model\n",
        "\n",
        "import re0\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "from collections import OrderedDict\n",
        "#from .utils import load_state_dict_from_url\n",
        "from torch import Tensor\n",
        "from torch.jit.annotations import List\n",
        "\n",
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
        "                                           growth_rate, kernel_size=1, stride=1,\n",
        "                                           bias=False)),\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                                           kernel_size=3, stride=1, padding=1,\n",
        "                                           bias=False)),\n",
        "        self.drop_rate = float(drop_rate)\n",
        "        self.memory_efficient = memory_efficient\n",
        "\n",
        "    def bn_function(self, inputs):\n",
        "        # type: (List[Tensor]) -> Tensor\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
        "        return bottleneck_output\n",
        "\n",
        "    # todo: rewrite when torchscript supports any\n",
        "    def any_requires_grad(self, input):\n",
        "        # type: (List[Tensor]) -> bool\n",
        "        for tensor in input:\n",
        "            if tensor.requires_grad:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @torch.jit.unused  # noqa: T484\n",
        "    def call_checkpoint_bottleneck(self, input):\n",
        "        # type: (List[Tensor]) -> Tensor\n",
        "        def closure(*inputs):\n",
        "            return self.bn_function(inputs)\n",
        "\n",
        "        return cp.checkpoint(closure, *input)\n",
        "\n",
        "    @torch.jit._overload_method  # noqa: F811\n",
        "    def forward(self, input):\n",
        "        # type: (List[Tensor]) -> (Tensor)\n",
        "        pass\n",
        "\n",
        "    @torch.jit._overload_method  # noqa: F811\n",
        "    def forward(self, input):\n",
        "        # type: (Tensor) -> (Tensor)\n",
        "        pass\n",
        "\n",
        "    # torchscript does not yet support *args, so we overload method\n",
        "    # allowing it to take either a List[Tensor] or single Tensor\n",
        "    def forward(self, input):  # noqa: F811\n",
        "        if isinstance(input, Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
        "            if torch.jit.is_scripting():\n",
        "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
        "\n",
        "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
        "        else:\n",
        "            bottleneck_output = self.bn_function(prev_features)\n",
        "\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
        "                                     training=self.training)\n",
        "        return new_features\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features):\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    r\"\"\"Densenet-BC model class, based on\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
        "                                padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "        ]))\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient\n",
        "            )\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features,\n",
        "                                    num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Sequential(\n",
        "        nn.Linear(num_features, 512),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 1)\n",
        "    )\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "model = DenseNet(growth_rate=32, block_config=(6, 12, 24),\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=2, memory_efficient=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Running Model with parameters"
      ],
      "metadata": {
        "id": "_pFrJW1GUAru"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nvdzdCC2_yX"
      },
      "outputs": [],
      "source": [
        "                            ###Setting Loss function along with a simple iteration of the output\n",
        "\n",
        "myloss = nn.BCEWithLogitsLoss()\n",
        "model.cuda()\n",
        "X, y = next(iter(train_dl))\n",
        "X , y = X.cuda(), y.cuda()\n",
        "print('X, y shapes: ', X.shape, y.shape)\n",
        "out = model(X).squeeze()\n",
        "print(out.shape)\n",
        "print(f'loss: {myloss(y, out.type(torch.DoubleTensor).cuda()).item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TWuz3ax-2AZ"
      },
      "outputs": [],
      "source": [
        "def function_timer(function):                 ##Functon to count the timme taken per epoch\n",
        "    \n",
        "    def wrapper(*args, **kwargs):\n",
        "        start    = time.time()\n",
        "        result   = function(*args, **kwargs)\n",
        "        duration = time.time() - start\n",
        "        \n",
        "        hours    = int(duration // 60**2)\n",
        "        minutes  = int((duration % 60**2) // 60)  \n",
        "        seconds  = int(duration % 60)\n",
        "        print(f'execution-time of function \"{function.__name__}\": {hours}h {minutes}m {seconds}s')\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the model"
      ],
      "metadata": {
        "id": "O3NAtzR_UZI9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXqxeP6e-twN"
      },
      "outputs": [],
      "source": [
        "@function_timer\n",
        "def train_model(net, train, validation, optimizer, device, max_epoch=100, verbose=False):\n",
        "    \"\"\"\n",
        "    This function returns nothing. The parametes of @net are updated in-place\n",
        "    and the error statistics are written to a global variable. This allows to\n",
        "    stop the training at any point and still have the results.\n",
        "  \n",
        "    @ net: a defined model - can also be pretrained\n",
        "    @ train, test: DataLoaders of training- and test-set\n",
        "    @ max_epoch: stop training after this number of epochs\n",
        "    \"\"\"\n",
        "    global error_df  # to track error log even when training aborted\n",
        "    error_df = pd.DataFrame(columns=['train_bce', 'train_acc', 'train_auc', 'val_bce', 'val_acc', 'val_auc'])\n",
        "  \n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "    net.to(device)\n",
        "  \n",
        "    print('epoch\\tLR\\ttr-BCE\\ttr-Acc\\ttr-AUC\\t\\tval-BCE\\tval-Acc\\tval-AUC')\n",
        "    for epoch in range(max_epoch):\n",
        "        net.train()\n",
        "        training_bce = training_acc = training_auc = 0\n",
        "    \n",
        "        for X, y in train:\n",
        "            \n",
        "            X , y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # prediction and error:\n",
        "            out  = net(X).squeeze()\n",
        "            \n",
        "            labels = y.detach().cpu().numpy()\n",
        "            probabilities = torch.sigmoid(out).detach().cpu().numpy()\n",
        "            predictions = probabilities.round()\n",
        "            loss = criterion(out.type(torch.DoubleTensor).cuda(), y)\n",
        "            \n",
        "            training_bce += loss.item()\n",
        "            training_acc += np.mean(labels == predictions) * 100\n",
        "            training_auc += roc_auc_score(y_true=labels, y_score=probabilities)\n",
        "\n",
        "            # update parameters:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        with torch.no_grad():  # no backpropagation necessary\n",
        "            net.eval()\n",
        "            validation_bce = validation_acc = validation_auc = 0\n",
        "\n",
        "            for X, y in validation:\n",
        "                X , y = X.to(device), y.to(device)\n",
        "\n",
        "                # prediction and error:\n",
        "                out  = net(X).squeeze()\n",
        "\n",
        "                labels = y.detach().cpu().numpy()\n",
        "                probabilities = torch.sigmoid(out).detach().cpu().numpy()\n",
        "                predictions = probabilities.round()\n",
        "\n",
        "                validation_bce += criterion(out.type(torch.DoubleTensor).cuda(), y).item()\n",
        "                validation_acc += np.mean(labels == predictions) * 100\n",
        "                validation_auc += roc_auc_score(y_true=labels, y_score=probabilities)\n",
        "    \n",
        "        # convert to batch loss:\n",
        "        training_bce   /= len(train)\n",
        "        training_acc   /= len(train)\n",
        "        training_auc   /= len(train)\n",
        "        \n",
        "        validation_bce /= len(validation)\n",
        "        validation_acc /= len(validation)\n",
        "        validation_auc /= len(validation)\n",
        "        scheduler.step()\n",
        "       \n",
        "        #torch.save(net.state_dict(), f'epoch{epoch}.pt')\n",
        "        error_stats = [training_bce, training_acc, training_auc, validation_bce, validation_acc, validation_auc]\n",
        "        error_df = error_df.append(pd.Series(error_stats, index=error_df.columns), ignore_index=True)\n",
        "        print('{}\\t{:.4f}\\t{:.4f}\\t{:.2f}\\t{:.4f}\\t\\t{:.4f}\\t{:.2f}\\t{:.4f}'.format(epoch, optimizer.param_groups[0]['lr'], *error_stats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFxz3BhH-4HP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.0005,momentum=0.9)  #5e-6)\n",
        "\n",
        "\n",
        "            ##Training the model\n",
        "\n",
        "            \n",
        "train_model(model,\n",
        "            train_dl,\n",
        "            valid_dl,\n",
        "            optimizer,\n",
        "            device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
        "            max_epoch=10 ,\n",
        "            verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attention HeatMap"
      ],
      "metadata": {
        "id": "kCtcHVohVFqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROVX3s5aI4fq"
      },
      "outputs": [],
      "source": [
        "class CAM(nn.Module):                               ##Class to get attention map as heat map\n",
        "    def __init__(self, model_to_convert, get_fc_layer=lambda m: m.classifier,score_fn=F.softmax, resize=True):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(*list(model_to_convert.children())[:-1])\n",
        "        self.fc = get_fc_layer(model_to_convert)\n",
        "        self.conv  =  nn.Conv2d(self.fc[0].in_features, self.fc[0].out_features, kernel_size=1)\n",
        "        self.conv.weight = nn.Parameter(self.fc[0].weight.data.unsqueeze(-1).unsqueeze(-1))\n",
        "        self.conv.bias = self.fc[0].bias\n",
        "        self.score_fn = score_fn\n",
        "        self.resize = resize\n",
        "        self.eval()\n",
        "        \n",
        "    def forward(self, x, out_size=None):\n",
        "        batch_size, c, *size = x.size()\n",
        "        feat = self.backbone(x)\n",
        "        cmap = self.score_fn(self.conv(feat))\n",
        "        if self.resize:\n",
        "            if out_size is None:\n",
        "                out_size = size\n",
        "            cmap = F.upsample(cmap, size=out_size, mode='bilinear')\n",
        "        pooled = F.adaptive_avg_pool2d(feat,output_size=1)\n",
        "        flatten = pooled.view(batch_size, -1)\n",
        "        cls_score = self.score_fn(self.fc(flatten))\n",
        "        weighted_cmap =  (cmap*cls_score.unsqueeze(-1).unsqueeze(-1)).sum(dim=1)\n",
        "        return cmap, cls_score, weighted_cmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxSbLbFUh4Pc"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import densenet121\n",
        "import torchvision.transforms as transforms\n",
        "                                                            #Transforming image based on paper\n",
        "%matplotlib inline\n",
        "target_size = (48,48)\n",
        "# normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
        "#                                  [0.229, 0.224, 0.225])\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(target_size),transforms.CenterCrop(target_size),\n",
        "                                transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwwNOSGHh8Yo"
      },
      "outputs": [],
      "source": [
        "cam = CAM(model)\n",
        "assert not cam.training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpVTC-pfjaB3"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"use gpu\")\n",
        "    cam = cam.cuda()\n",
        "    def to_var(x, requires_grad=False, volatile=False):\n",
        "        return Variable(x.cuda(), requires_grad=requires_grad, volatile=volatile)\n",
        "else:\n",
        "    def to_var(x, requires_grad=False, volatile=False):\n",
        "        return Variable(x, requires_grad=requires_grad, volatile=volatile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vGC1ygejtl7"
      },
      "outputs": [],
      "source": [
        "\n",
        "img = Image.open('/content/img_align_celeba/img_align_celeba/111111.jpg')\n",
        "img_v = to_var(transform(img).unsqueeze(0),volatile=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0MXqkrXjatR"
      },
      "outputs": [],
      "source": [
        "cmap, score, weighted_cmap = cam(img_v)#prodTensor = color_map_tensor *orb_image_tensor\n",
        "#prodasNp = (prodTensor.permute(2, 0, 1) * 255).to(torch.uint8).numpy()\n",
        "#prodasNp\n",
        "print(cmap.size())\n",
        "print(score.size())\n",
        "print(weighted_cmap.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6pW_BGZljcs"
      },
      "source": [
        "##Combining LFE and AGM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQAU0RHUkNqN"
      },
      "outputs": [],
      "source": [
        "background = np.array(img.resize(target_size))\n",
        "color_map =  weighted_cmap.data.cpu().numpy()[0]\n",
        "# color_map = cmap.data.cpu().numpy()[0,55]\n",
        "color_map.shape\n",
        "#plt.imshow(background)\n",
        "#plt.imshow(color_map,alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoMyJq2ihMNp"
      },
      "outputs": [],
      "source": [
        "#type(color_map)\n",
        "#type(orb_image)\n",
        "color_map_tensor = transforms.ToTensor()(color_map)\n",
        "orb_image_tensor = transforms.ToTensor()(orb_image)\n",
        "print(type(color_map))\n",
        "print((color_map_tensor))\n",
        "resized_img_tensor = transforms.ToTensor()(resized_img)\n",
        "print(type(resized_img_tensor))\n",
        "prodTensor = color_map_tensor *orb_image_tensor \n",
        "\n",
        "prodasNp = (prodTensor.permute(1, 2, 0) * 255).to(torch.uint8).numpy()\n",
        "\n",
        "plt.imshow(prodasNp)\n",
        "# color_map_tensor.size()\n",
        "# orb_image_tensor.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP2pRgWpTyCo"
      },
      "source": [
        "## Mouth Faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6brIzeld1Q6S"
      },
      "outputs": [],
      "source": [
        "#Extracting mouth attribute images\n",
        "class MouthFacesDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \n",
        "        self.landmarks_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "    \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        #print(\"GET!\")\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        print(self.landmarks_frame.iloc[idx, 0])\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.landmarks_frame.iloc[idx, 0])\n",
        "\n",
        "        image = cv2.imread(img_name)\n",
        "\n",
        "        sample = image\n",
        "\n",
        "        sample = Image.fromarray(np.uint8(sample)).convert('RGB')\n",
        "        \n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "mouth_dataset = MouthFacesDataset(csv_file = '/content/list_attr_celeba.csv', \n",
        "                                root_dir= '/content/img_align_celeba/img_align_celeba')\n",
        "print(mouth_dataset)\n",
        "landmarks_frame = pd.read_csv('/content/list_attr_celeba.csv')                 # read the csv file with attributes\n",
        "mouth_images_list  = landmarks_frame.iloc[:,[7,22,32]]                    # extract the column with eye attributes-(7-Big_Lips,22-Mouth_Slightly_Open,32-Smiling)\n",
        "#print(mouth_images_list)\n",
        "mouth_images_list_1 =  mouth_images_list[mouth_images_list == 1]     # select only the indices for mouth (any of the four attribute val = 1)\n",
        "#print(mouth_images_list)\n",
        "\n",
        "mouth_images_list_1 = mouth_images_list_1.dropna( axis=0,\n",
        "                how='all')\n",
        "print(mouth_images_list_1)\n",
        "mouth_images_index_1 = mouth_images_list_1.index      # corresponding image indices\n",
        "#print(mouth_images_index)\n",
        "\n",
        "\n",
        "mouth_images_list_2 =  mouth_images_list[mouth_images_list != 1] \n",
        "\n",
        "mouth_images_list_2 = mouth_images_list_2.dropna( axis=0,\n",
        "                how='any')\n",
        "mouth_images_index_2 = mouth_images_list_2.index \n",
        "print(mouth_images_list_2)\n",
        "mouth_dataset[0] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4eYSH9W9CZI"
      },
      "source": [
        "## MouthPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGens2ta5-qw"
      },
      "outputs": [],
      "source": [
        "#Extracting mouth attribute images\n",
        "\n",
        "\n",
        "class MouthPNDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, df, root_dir, transform=None):\n",
        "        \n",
        "        self.landmarks_frame = df\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_labels = []\n",
        "        for i in range(len(self.landmarks_frame)):\n",
        "          self.image_labels.append(self.landmarks_frame['imtype'].values[i])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.landmarks_frame)\n",
        "    \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        #print(\"GET!\")\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        num = self.landmarks_frame.iloc[idx, 3]\n",
        "        \n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.landmarks_frame.iloc[idx, 4])\n",
        "\n",
        "        # image_data = Image.open(img_name).convert(\"RGB\") # Convert image to RGB channels\n",
        "        \n",
        "        # TODO: Image augmentation code would be placed here\n",
        "        \n",
        "        # Resize and convert image to torch tensor \n",
        "        # image_data = self.image_transformation(image_data)\n",
        "        image = cv2.imread(img_name)\n",
        "\n",
        "        sample = image\n",
        "\n",
        "        sample = Image.fromarray(np.uint8(sample)).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        labels = torch.Tensor(self.image_labels)\n",
        "\n",
        "        return sample, labels[idx]\n",
        "\n",
        "df = mouth_images_list\n",
        "# df.head()\n",
        "df['imtype'] = 0\n",
        "df.iloc[mouth_images_index_1, df.columns.get_loc('imtype')] = 1\n",
        "\n",
        "pdf = df[df['imtype'] == 1]\n",
        "pdf = pdf.sample(n=15000)\n",
        "\n",
        "ndf = df[df['imtype'] == 0]\n",
        "ndf = ndf.sample(n=15000)\n",
        "image_df = pdf.append(ndf)\n",
        "image_df['image_id'] = [str(x+1).zfill(6) + '.jpg' for x in image_df.index]\n",
        "image_df = image_df.sample(frac=1)\n",
        "val_df = df.sample(n=3000)\n",
        "\n",
        "val_df['image_id'] = [str(x+1).zfill(6) + '.jpg' for x in val_df.index]  \n",
        "\n",
        "image_df.head()\n",
        "\n",
        "mouthpn_dataset= MouthPNDataset(df = image_df, \n",
        "                                root_dir= '/content/img_align_celeba/img_align_celeba')\n",
        "val_dataset = MouthPNDataset(df = val_df, \n",
        "                                root_dir= '/content/img_align_celeba/img_align_celeba')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEghR8QmHgmg"
      },
      "outputs": [],
      "source": [
        "image_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvtt8aVj_XVa"
      },
      "source": [
        "## Datagen for mouth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfCCIWJd5_RH"
      },
      "outputs": [],
      "source": [
        "def get_train_dataset_mouth(dataframe: object,ang , transformation: bool=True) -> (object, object):\n",
        "    '''\n",
        "        Returns an object on FERDataset class\n",
        "        Input:\n",
        "            dataframe: object -> DataFrame object containing the whole data\n",
        "            transformation: bool [optional] ->  Apply transformations\n",
        "    '''\n",
        "\n",
        "    # extracts rows specific to Training, PublicTest\n",
        "    #dataframe = dataframe.loc[dataframe.Usage.isin(['Training', 'PublicTest'])]\n",
        "    # drop Usage column as it's no longer needed    \n",
        "    #dataframe = dataframe.drop('Usage', axis=1)\n",
        "\n",
        "    # split dataset into training and validation set\n",
        "    np.random.seed(42)  \n",
        "    #msk = np.random.rand(len(dataframe)) < 0.8\n",
        "\n",
        "    #train_df = dataframe[msk].reset_index()\n",
        "    #val_df = dataframe[~msk].reset_index()\n",
        "\n",
        "    # get transformations\n",
        "    if transformation:\n",
        "        train_tfms, valid_tfms = image_transformations(ang)\n",
        "    else:\n",
        "        train_tfms, valid_tfms = None, None\n",
        "    \n",
        "    # fetch dataset\n",
        "    train_mouth_ds = MouthPNDataset(df = image_df, \n",
        "                                root_dir= '/content/img_align_celeba/img_align_celeba', transform=train_tfms)\n",
        "    val_mouth_ds = MouthPNDataset(df = val_df, \n",
        "                                root_dir= '/content/img_align_celeba/img_align_celeba', transform=valid_tfms)\n",
        "    print(train_mouth_ds[0])\n",
        "    return train_mouth_ds, val_mouth_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BnakRYSW50d"
      },
      "outputs": [],
      "source": [
        "def get_train_dataloader_mouth(dataframe: object, transformation=True, batch_size: int=16) -> (object, object):\n",
        "    '''\n",
        "        Returns train and test dataloaders.\n",
        "        Input:\n",
        "            dataframe: dataset DataFrame object\n",
        "            batch_size: [optional] int\n",
        "        Output:\n",
        "            train_dl: train dataloader object\n",
        "            valid_dl: validation dataloader object\n",
        "    '''\n",
        "    # fetech train and validation dataset\n",
        "    train_mouth_ds, valid_mouth_ds = get_train_dataset_mouth(dataframe, -15, transformation=transformation)\n",
        "    for i in range(-2, 4):\n",
        "      t, _ = get_train_dataset_mouth(dataframe, 5*i, transformation=transformation)\n",
        "      train_mouth_ds = ConcatDataset([train_mouth_ds,t])\n",
        "    #print(len(train_ds))\n",
        "    train_mouth_dl = DataLoader(train_mouth_ds, batch_size, shuffle=True, \n",
        "                     num_workers=0, pin_memory=True)\n",
        "    valid_mouth_dl = DataLoader(valid_mouth_ds, batch_size, \n",
        "                    num_workers=0, pin_memory=True)\n",
        "    \n",
        "    return train_mouth_dl, valid_mouth_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYF8A2_hXG7k"
      },
      "outputs": [],
      "source": [
        "train_mouth_dl, valid_mouth_dl = get_train_dataloader_mouth(eye_dataset,batch_size=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9fUJle45_ax"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.0005,momentum=0.9)  #5e-6)\n",
        "\n",
        "\n",
        "\n",
        "train_model(model,\n",
        "            train_mouth_dl,\n",
        "            valid_mouth_dl,\n",
        "            optimizer,\n",
        "            device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
        "            max_epoch=10 ,\n",
        "            verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Heatmap for mouth"
      ],
      "metadata": {
        "id": "cUEJwQS8XrTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cam = CAM(model)\n",
        "assert not cam.training "
      ],
      "metadata": {
        "id": "Q1OrflbqXahf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RAmQTGAXpkP"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"use gpu\")\n",
        "    cam = cam.cuda()\n",
        "    def to_var(x, requires_grad=False, volatile=False):\n",
        "        return Variable(x.cuda(), requires_grad=requires_grad, volatile=volatile)\n",
        "else:\n",
        "    def to_var(x, requires_grad=False, volatile=False):\n",
        "        return Variable(x, requires_grad=requires_grad, volatile=volatile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouQHMowwXpkQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "img = Image.open('/content/img_align_celeba/img_align_celeba/111111.jpg')\n",
        "img_v = to_var(transform(img).unsqueeze(0),volatile=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSQzRS-dXpkQ"
      },
      "outputs": [],
      "source": [
        "cmap, score, weighted_cmap = cam(img_v)#prodTensor = color_map_tensor *orb_image_tensor\n",
        "#prodasNp = (prodTensor.permute(2, 0, 1) * 255).to(torch.uint8).numpy()\n",
        "#prodasNp\n",
        "print(cmap.size())\n",
        "print(score.size())\n",
        "print(weighted_cmap.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkEjqT87NqeT"
      },
      "outputs": [],
      "source": [
        "color_map_tensor = transforms.ToTensor()(color_map)\n",
        "orb_image_tensor = transforms.ToTensor()(orb_image)\n",
        "print(type(color_map))\n",
        "print((color_map_tensor))\n",
        "resized_img_tensor = transforms.ToTensor()(resized_img)\n",
        "print(type(resized_img_tensor))\n",
        "prodTensor = color_map_tensor *orb_image_tensor \n",
        "\n",
        "prodasNp = (prodTensor.permute(1, 2, 0) * 255).to(torch.uint8).numpy()\n",
        "\n",
        "plt.imshow(prodasNp)\n",
        "# color_map_tensor.size()\n",
        "# orb_image_tensor.size()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DL-Project-Final",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}